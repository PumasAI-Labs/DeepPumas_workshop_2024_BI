---
title: Scientific machine learning with DeepPumas
execute:
  error: false
  cache: true
author:
  - Niklas Korsbo
date: today
julia:
  exeflags:
    - '--project=..'
logo: 'https://pumas-assets.s3.amazonaws.com/CompanyLogos/DeepPumas/RGB/SVG/DeepPumas+Primaryv.svg'
format:
  revealjs:
    width: 1200
    height: 800
    auto-stretch: false
    html-math-method: mathjax
    transition: none
    center: true
    scrollable: false
    progress: true
    slide-number: true
    smaller: false
    theme: custom_moon.scss
    css: style.css
    mermaid:
      theme: dark
      flowchart:
        htmlLabels: true
    fig-align: center
engine: julia 
---

# Introduction

```{=html}
<style>
.MathJax {
    font-size: 80% !important;
}
</style>
```

```{julia}
#| warning: false
#| output: false
using DeepPumas
using StableRNGs
using PumasPlots
using CairoMakie
using Serialization
using Latexify
using Markdown
using QuartoTools
set_theme!(deep_dark(); backgroundcolor=:transparent, fontsize=45)

Latexify.set_default(starred=true, convert_unicode=false)
```


##


![&nbsp;](image.png){width=50% align="center"}


:::: {.columns style="font-size: 70%"}
::: {.column width=0.4}
Machine learning

::: {.callout-tip appearance="minimal"}
- Data-driven model discovery
- Finds unintuitive relationships
- Handles complex data
:::

::: {.callout-important appearance="minimal"}
- Lacks scientific understanding
- Requires big data
- Inscrutable  
:::
:::
  

::: {.column width=0.4}
Scientific modelling

::: {.callout-tip appearance="minimal"}
- Encodes scientific understanding
- Data-efficient
- Interpretable
- Simple counterfactuals
:::

::: {.callout-important appearance="minimal"}
- Labor intensive
- Misses unintuitive relationships
- Hard to utilize complex data
:::
:::
::::
  

## {.center}
![](image-3.png)

##

![](image-6.png)

##

![&nbsp;](image-7.png){width=70%}

##

![&nbsp;](image-1.png){width=70%}

```{julia}
#| output: false
pkmodel = @model begin
  @param begin
    tvKa in RealDomain(; lower=0)
    tvCL in RealDomain(; lower=0)
    tvVc in RealDomain(; lower=0)
    Ω in PDiagDomain(3)
    σ in RealDomain(; lower=0)
  end
  @random η ~ MvNormal(Ω)
  @pre begin
    Ka = tvKa * exp(η[1])
    CL = tvCL * exp(η[2])
    Vc = tvVc * exp(η[3])
  end
  @dynamics Depots1Central1
  @derived Concentration ~ @. Normal(Central/Vc, Central/Vc * σ)
end

param = (; tvKa = 2., tvCL=3., tvVc=1., Ω = [0.5, 0.5, 0.5], σ=0.1)
rng = StableRNG(1)
sims = map(1:20) do i 
  _subj = Subject(; id=i, events = DosageRegimen(1, ii=1, addl=2))
  sim = simobs(pkmodel, _subj, param; rng, obstimes = 0:0.2:4)
end
pop = Subject.(sims)
```

## {.smaller}

:::: {.columns}

::: {.column width="50%"}

**[Nonlinear Mixed Effects]{.att}**

::: {.fragment fragment-index=3 .fade-out}
::: {.absolute top=300, right=10, width=600}

```{julia}

sim = (; label="Data", markersize=15, linewidth=1, color=(:white, 0.6),)
plt = plotgrid(sims[1:1]; sim)
```

:::
:::

::: {.fragment fragment-index=3}

::: {.absolute top=500, right=100, width="600"}

```{julia}
foreach(sims[2:4]) do _sim
  plotgrid!(plt, [_sim]; sim)
end
plt
```


:::
:::

:::

::: {.column width="50%"}


::: {.fragment fragment-index=5}
Typical values

$$
tvKa, \; tvCL, \; tvVc, \; Ω, \; σ
$$

Covariates
$$
Age, \; Weight
$$

Random effects

$$
η \sim MvNormal(Ω)
$$

:::
  
::: {.fragment fragment-index=4}
Individual parameters

\begin{align*}
Ka_i &= tvKa \cdot e^{η_{i,1}} \\
CL_i &= tvCL \cdot e^{η_{i,2}} \\
Vc_i &= tvVc \cdot e^{η_{i,3}}
\end{align*}
:::

::: {.fragment fragment-index=1}
Dynamics

```{julia}
#| output: asis
latexify(pkmodel, :dynamics)
```

:::
::: {.fragment fragment-index=2}
Error model
$$
Concentration(t) \sim Normal\left(\frac{Central(t)}{Vc}, \frac{Central(t)}{Vc} \cdot σ\right)
$$
:::




:::
::::

---

```{julia}
#| output: false
using Flux
using CairoMakie
using Colors

ninput = 1
nhidden = 6
act=tanh

opt = Adam()
X = permutedims(collect(range(0, stop=1, length=301)))
Ys = [X .^ 2 .+ X, 2 .* X ./ (0.3 .+ X), sin.(2π .* X) .+ 1, exp2.(X)]

##
nnf = Flux.Chain(Dense(1,nhidden, act), Dense(nhidden, nhidden, act), Dense(nhidden, 1))
Ŷ = Observable(vec(Ys[end]))
Y = Observable(vec(Ys[1]))

fig, ax = lines(vec(X), Y; linewidth=6, axis=(ylabel="Output", xlabel="x"), label="f(x)", figure=(; resolution=(400,400), fontsize=25, backgroundcolor=colorant"#002b36"))
lines!(vec(X), Ŷ, label="NN(x)", linewidth=6)
Legend(fig[0,:], ax, orientation=:horizontal, framevisible=false)
fig
Ys = [X .^ 2 .+ X, 2 .* X ./ (0.3 .+ X), sin.(2π .* X) .+ 1, exp2.(X)]
nframes = 400
opt_state = Flux.setup(opt, nnf)
record(fig, "nn_demo_test.mp4", 1:nframes; framerate=30) do frame
  _Y = Ys[min(round(Int, frame ÷ (nframes/length(Ys))) + 1, end)]
  Y[] = vec(_Y)
  Ŷ[] = vec(nnf(X))
  
  steps_per_y = nframes / length(Ys)
  
  for j in 1:round(Int, 50 / steps_per_y * (frame%steps_per_y))
  grads = gradient(m -> Flux.mse(m(X), _Y), nnf)
    Flux.Optimise.update!(opt_state, nnf, grads[1])
  end
end 
```

## Neural networks {.smaller}

:::: {.columns}
::: {.column width="50%"}

[Information processing mechanism]{.att}

- Loosely based on neurons

![&nbsp;](image-4.png){fig-align="center"}

- Mathematically just a function!
- Usable anywhere you'd use a function!

:::
::: {.column width="50%"}

[Univeral approximators!]{.att}

![](nn_demo_test.mp4){loop="true" autoplay="true" muted="true"}

- Approximate *any* function
- Functional form tuned by parameters

:::
::::

## {.smaller}

:::: {.columns}

::: {.column width="50%"}

**[Deep Nonlinear Mixed Effects]{.att}**

::: {.absolute top=500, right=100, width="600"}

```{julia}
plt
```


:::
:::

::: {.column width="50%"}


Typical values

$$
tvKa, \; tvCL, \; tvVc, \; Ω, \; σ
$$

Covariates
$$
Age, \; Weight
$$

Random effects

$$
η \sim MvNormal(Ω)
$$

  
Individual parameters

\begin{align*}
Ka_i &= tvKa \cdot e^{η_{i,1}} \\
CL_i &= tvCL \cdot e^{η_{i,2}} \\
Vc_i &= tvVc \cdot e^{η_{i,3}}
\end{align*}

::: {.fragment}
![](image-8.png){.absolute top=150 right=90 width=110}
:::
::: {.fragment}
![](image-4.png){.absolute top=380 right=90 width=100}
:::
::: {.fragment}
![](image-4.png){.absolute bottom=140 right=80 width=130}
:::

Dynamics

```{julia}
latexify(pkmodel, :dynamics)
```

Error model
$$
Concentration(t) \sim Normal\left(\frac{Central(t)}{Vc}, \frac{Central(t)}{Vc} \cdot σ\right)
$$



:::
::::

## Ok, all good? Crystal clear? That was a short workshop!

::: {.fragment}
Well...

- What's this business of combining differential equations and ML?
- What does random effects do and how do we train such a model?
- How does random effects interact with machine learning?
:::

# Neural-embedded dynamical systems {.smaller}

2018 - "Neural Ordinary Differential Equations", Chen et al.

::: {.fragment fragment-index=1}
2020 - "Universal Differential Equations for Scientific Machine Learning", Rackauckas et al.
:::

:::: {.columns}
::: {.column width="33%"}

[Neural ODE]{.att}

$$
\frac{d\mathbf{X}}{dt} = NN(\mathbf{X}(t), t)
$$

::: {style="font-size: 100%"}
Use a differential equation solvers as a scaffold for continuous time, continuous depth neural networks. 

Similar to recurrent neural networks and ResNets

:::
:::
::: {.column width="33%"}

::: {.fragment fragment-index=1}
[Universal Differential Equations (UDE)]{.att}

\begin{align*}
\frac{dx}{dt} &= x \cdot y - NN(x)\\
\frac{dy}{dt} &= p - x \cdot y
\end{align*}

Insert universal approximators (like NNs) to capture terms in dynamical systems. 
:::

:::
::: {.column width="33%"}
::: {.fragment fragment-index=2}

[Scientific Machine Learning (SciML)]{.att}

An abstract concept of mixing scientific modeling with machine learning. 

:::
:::
::::

## Encoded knowledge {.smaller}

:::: {.columns}

::: {.column width="50%"}

::: {.att}
$$
\begin{aligned}
\frac{dDepot}{dt} &= NN(Depot, Central, R)[1]\\
\frac{dCentral}{dt} &= NN(Depot, Central, R)[2]\\
\frac{dR}{dt} &= NN(Depot, Central, R)[3]
\end{aligned}
$$
:::

- Number of states

::: {.fragment}

::: {.att}
$$
\begin{aligned}
\frac{dDepot}{dt} &= - NN_1(Depot)\\
\frac{dCentral}{dt} &= NN_1(Depot) - NN_2(Central)\\
\frac{dR}{dt} &= NN_3(Central, R)
\end{aligned}
$$
:::

- Number of states
- Dependecies
- Conservation

:::
:::

::: {.column width="50%"}

::: {.fragment}

::: {.att}
$$
\begin{aligned}
\frac{dDepot}{dt} &= - K_a \cdot Depot\\
\frac{dCentral}{dt} &= K_a \cdot Depot - CL/V_c \cdot Central\\
\frac{dR}{dt} &= NN_3(Central, R)
\end{aligned}
$$
:::

- Explicit knowledge of some terms

:::
  
::: {.fragment}
[
$$
\begin{aligned}
\frac{dDepot}{dt} &= - K_a \cdot Depot\\
\frac{dCentral}{dt} &= K_a \cdot Depot - CL/V_c \cdot Central\\
\frac{dR}{dt} &= k_{in} \cdot (1 + NN(Central)) - k_{out} \cdot R
\end{aligned}
$$
]{.att}

- Precise position of the unknown function
- Precise infor to the unknown input
- Lots of knowledge!

:::
:::


::::

## UDEs - pretty simple, really {.smaller}


![
Mathematically: just a function!
](image-4.png){fig-width="40%" fig-cap-alignment="center"}


NNs useable anywhere you'd use a function!

- Decide where in the dynamics you have an unknown function.
- Decide what inputs this function may have.
- Fit [everything]{.underline} in concert

The only hard part is building software for robust fitting - but we've got you covered there!

# Random effects and machine learning


## Fitting with random effects
[Their effect is largely determined by how they contribute to the loss function of a model fit]{style="font-size: 60%"}

[Joint likelihood]{.att}

Find fixed effects, θ and random effects, η that, given covariates x, and Data, maximize the likelihood $L$.

[
$$
L(θ, η | Data, x)
$$
]{.att style="font-size=140%"}

Maximizes the conditional probability that the data comes from the model $P(Data | θ, η, x)$.

Essentially minimises a distance between predicted and observed values.

## Fitting with random effects
[Their effect is largely determined by how they contribute to the loss function of a model fit]{style="font-size: 60%"}

[Marginal likelihood]{.att}

Maginalize the random effects, weighted by its prior.

[
$$
L_m(θ | Data, x) = \int L(θ, η | Data, x) P(η | θ) dη
$$
]{.att}

Maximizes the weighted average probability for all possible random effect values. 

Penalizes sensitivity to the precise random effect values.

## Fitting with random effects

```{julia}
#| fig-cap: '&nbsp;'

f(x; s=1, μ=0, σ=1) = @. s * exp(- (x-μ)^2/(2σ^2)) 
p(x; σ = 1.1) = f(x; σ, s=1/(σ*sqrt(2π)))

x = -3:0.01:3

fig = Figure(; size=(1200, 1200), fontsize=25)
ylabels=[L"p(Y|θ,η,x)", L"p_{p}(η|θ)", L"p(Y|θ,η,x) \cdot p_{p}(η|θ)"]
axes = [Axis(fig[i,1], ylabel=ylabels[i]) for i in 1:3]
_l1 = lines!(axes[1], x, f(x; s=0.7, μ=0.3, σ=0.3))
_l2 = lines!(axes[1], x, f(x; s=0.9, μ=-1.5, σ=0.1))
_l3 = lines!(axes[1], x, f(x; s=0.4, μ=1., σ=0.9))

lines!(axes[2], x, p(x), color=(:white, 0.8))

lines!(axes[3], x, p(x) .* f(x; s=0.7, μ=0.3, σ=0.3))
band!(axes[3], x, 0, p(x) .* f(x; s=0.7, μ=0.3, σ=0.3))
lines!(axes[3], x, p(x) .* f(x; s=0.9, μ=-1.5, σ=0.1))
band!(axes[3], x, 0, p(x) .* f(x; s=0.9, μ=-1.5, σ=0.1), alpha=0.1)
lines!(axes[3], x, p(x) .* f(x; s=0.4, μ=1., σ=0.9))
band!(axes[3], x, 0, p(x) .* f(x; s=0.4, μ=1., σ=0.9), alpha=0.4)
Label(fig[end+1, :], "η", tellwidth=false)
Legend(fig[0,:], [_l1, _l2, _l3], ["Patient 1", "Patient 2", "Patient 3"], tellwidth=false, orientation=:horizontal)
fig
```

## Marginal likelihood in reality

$$
L_m(θ | Data, x) = \int L(θ, η | Data, x) P(η | θ) dη
$$

Often intractable.

Direct approximate maximization

- Laplace, FOCE, FO
- Bayesian MCMC

Indirect maximization

- Expectation maximization (EM)
  - Stochastic appromimation (SAEM)
  - Variational inference (VIEM)

# Mixed effect machine learning


```{julia}
menet = @model begin
  @param begin
    NN ∈ MLPDomain(3, 7, 7, (1, identity); reg=L2())
    σ ∈ RealDomain(; lower=0)
  end
  @random η ~ MvNormal(2, 0.1)
  @derived Concentration ~ map(t) do _t
    Normal(NN(_t, η)[1], σ)
  end
end
pop_nodose = Subject.(pop; events=nothing)
fpm = fit(menet, pop_nodose[1:14], init_params(menet), MAP(FOCE()); optim_options=(;iterations=1000, show_trace=false))
plotgrid(predict(fpm, pop_nodose[15:end]; obstimes=0:0.01:4); figure = (; size = (1000,600), fontsize=25))
```


# Generate synthetic data

## Defining synethetic data model {auto-animate="true"}

```julia
datamodel = @model begin
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + EFF) - Kout * R
    end
end
```

## Defining synethetic data model {auto-animate="true"}
```julia
datamodel = @model begin
    @vars begin
cp = abs(Central / Vc)
        EFF = Smax * cp^n / (SC50^n + cp^n)
    end
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + EFF) - Kout * R
    end
end
```


## Defining synethetic data model {auto-animate="true"}
```julia
datamodel = @model begin
    @vars begin
        cp = abs(Central / Vc)
        EFF = Smax * cp^n / (SC50^n + cp^n)
    end
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + EFF) - Kout * R
    end
    @derived begin
        yPK ~ @. Normal(Central ./ Vc, σ_pk)
        yPD ~ @. Normal(R, σ_pd)
    end
end
```


## Defining synethetic data model {auto-animate="true"}
```julia
datamodel = @model begin
    @init R = Kin / Kout
    @vars begin
        cp = abs(Central / Vc)
        EFF = Smax * cp^n / (SC50^n + cp^n)
    end
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + EFF) - Kout * R
    end
    @derived begin
        yPK ~ @. Normal(Central ./ Vc, σ_pk)
        yPD ~ @. Normal(R, σ_pd)
    end
end
```

## Defining synethetic data model {auto-animate="true"}
```julia
datamodel = @model begin 
    @pre begin
        Smax = tvSmax * exp(η[1]) + 3 * c1 / (10.0 + c1)
        SC50 = tvSC50 * exp(η[2] + 0.3 * (c2 / 20)^0.75)
        Ka = tvKa * exp(η[3] + 0.3 * c3 * c4)
        Vc = tvVc * exp(η[4] + 0.3 * c3)
        Kout = tvKout * exp(η[5] + 0.3 * c5 / (c6 + c5))
        Kin = tvKin
        CL = tvCL
        n = tvn
    end
    @init R = Kin / Kout
    @vars begin
        cp = abs(Central / Vc)
        EFF = Smax * cp^n / (SC50^n + cp^n)
    end
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + EFF) - Kout * R
    end
    @derived begin
        yPK ~ @. Normal(Central ./ Vc, σ_pk)
        yPD ~ @. Normal(R, σ_pd)
    end
end
```

## Defining synethetic data model {auto-animate="true"}
```julia
datamodel = @model begin 
    @param begin
        tvKa ∈ RealDomain(; lower = 0, init = 0.5)
        tvCL ∈ RealDomain(; lower = 0)
        tvVc ∈ RealDomain(; lower = 0)
        tvSmax ∈ RealDomain(; lower = 0, init = 0.9)
        tvn ∈ RealDomain(; lower = 0, init = 1.5)
        tvSC50 ∈ RealDomain(; lower = 0, init = 0.1)
        tvKout ∈ RealDomain(; lower = 0, init = 1.2)
        tvKin ∈ RealDomain(; lower = 0, init = 1.2)
        Ω ∈ PDiagDomain(; init = fill(0.05, 5))
        σ_pk ∈ RealDomain(; lower = 0, init = 1e-1)
        σ_pd ∈ RealDomain(; lower = 0, init = 1e-1)
    end
    @random η ~ MvNormal(Ω)
    @covariates c1 c2 c3 c4 c5 c6
    @pre begin
        Smax = tvSmax * exp(η[1]) + 3 * c1 / (10.0 + c1)
        SC50 = tvSC50 * exp(η[2] + 0.3 * (c2 / 20)^0.75)
        Ka = tvKa * exp(η[3] + 0.3 * c3 * c4)
        Vc = tvVc * exp(η[4] + 0.3 * c3)
        Kout = tvKout * exp(η[5] + 0.3 * c5 / (c6 + c5))
        Kin = tvKin
        CL = tvCL
        n = tvn
    end
    @init R = Kin / Kout
    @vars begin
        cp = abs(Central / Vc)
        EFF = Smax * cp^n / (SC50^n + cp^n)
    end
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + EFF) - Kout * R
    end
    @derived begin
        yPK ~ @. Normal(Central ./ Vc, σ_pk)
        yPD ~ @. Normal(R, σ_pd)
    end
end
```

## Defining synethetic data model {auto-animate="true"}

```{julia}
#| echo: true
#| output: false
#| auto-animate: true

datamodel = @model begin
    @param begin
        tvKa ∈ RealDomain(; lower = 0, init = 0.5)
        tvCL ∈ RealDomain(; lower = 0)
        tvVc ∈ RealDomain(; lower = 0)
        tvSmax ∈ RealDomain(; lower = 0, init = 0.9)
        tvn ∈ RealDomain(; lower = 0, init = 1.5)
        tvSC50 ∈ RealDomain(; lower = 0, init = 0.1)
        tvKout ∈ RealDomain(; lower = 0, init = 1.2)
        tvKin ∈ RealDomain(; lower = 0, init = 1.2)
        Ω ∈ PDiagDomain(; init = fill(0.05, 5))
        σ_pk ∈ RealDomain(; lower = 0, init = 1e-1)
        σ_pd ∈ RealDomain(; lower = 0, init = 1e-1)
    end
    @random η ~ MvNormal(Ω)
    @covariates c1 c2 c3 c4 c5 c6
    @pre begin
        Smax = tvSmax * exp(η[1]) + 3 * c1 / (10.0 + c1)
        SC50 = tvSC50 * exp(η[2] + 0.3 * (c2 / 20)^0.75)
        Ka = tvKa * exp(η[3] + 0.3 * c3 * c4)
        Vc = tvVc * exp(η[4] + 0.3 * c3)
        Kout = tvKout * exp(η[5] + 0.3 * c5 / (c6 + c5))
        Kin = tvKin
        CL = tvCL
        n = tvn
    end
    @init R = Kin / Kout
    @vars begin
        cp = abs(Central / Vc)
        EFF = Smax * cp^n / (SC50^n + cp^n)
    end
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + EFF) - Kout * R
    end
    @derived begin
        yPK ~ @. Normal(Central ./ Vc, σ_pk)
        yPD ~ @. Normal(R, σ_pd)
    end
end
```

---

The model syntax should make it fairly simple for the experienced NLME modeler to understand what is going on but we can also automatically generate even more readable equations from our models:

```{julia}
#| echo: true
latexify(datamodel, :dynamics)
```
